{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters\n",
    "* BUFFER_SIZE - experience replay buffer size\n",
    "* BATCH_SIZE - minibatch size to retirieve from experience replay\n",
    "* GAMMA - discount rate\n",
    "* TAU - parameter for soft update that determines how fast target network is merged into local\n",
    "* LR_ACTOR - learning rate for actor network\n",
    "* LR_CRITIC - learning rate for critic network\n",
    "* WEIGHT_DECAY - L2 regularization weight decay factor\n",
    "* LEARN_EVERY - determines timestep interval to learn networks\n",
    "* LEARN_NUM - number of learning procedures \n",
    "* OU_SIGMA - Ornstein-Uhlenbeck noise parameter\n",
    "* OU_THETA - Ornstein-Uhlenbeck noise parameter\n",
    "* EPSILON - parameter that controls noice effect for exploration/exploitation\n",
    "* EPSILON_DECAY - decay rate for EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 128       \n",
    "GAMMA = 0.99          \n",
    "TAU = 1e-3             \n",
    "LR_ACTOR = 1e-3      \n",
    "LR_CRITIC = 1e-3      \n",
    "WEIGHT_DECAY = 0     \n",
    "LEARN_EVERY = 20      \n",
    "LEARN_NUM = 10      \n",
    "OU_SIGMA = 0.2        \n",
    "OU_THETA = 0.15      \n",
    "EPSILON = 1.0         \n",
    "EPSILON_DECAY = 1e-6 \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class\n",
    "A core class that implement the actual DDPG algorithm.\n",
    "\n",
    "In a constructor it creates 2 pairs of network (1 for actor and another for critic), noise class for more exploration and experience replay buffer.\n",
    "\n",
    "Important functions:\n",
    "* `step` - records an state-action-reward tuple into experice replay buffer and starts network training of specified interval\n",
    "* `act` - retrieves action for a state from action local network\n",
    "* `learn` - does the actual learning by implementing DDPG algorithm:\n",
    "\t* step 1: getting predicted next-state actions and Q values from target models\n",
    "\t* step 2: computing Q targets for current states \n",
    "\t* step 3: computing critic loss\n",
    "\t* step 4: minimizing critic loss and traing the network\n",
    "\t* step 5: computing actor loss (with '-' sign as this is gradient ascent\n",
    "\t* step 6: minimizing actor loss and traing the network\n",
    "\t* step 7: applying soft update and slowly mering local networks into target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.epsilon = EPSILON\n",
    "\n",
    "        # actor networks\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # critic networks\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "\n",
    "    def load_actor_model(self, path):\n",
    "        self.actor_local.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def load_critic_model(self, path):\n",
    "        self.critic_local.load_state_dict(torch.load(path))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done, timestep):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % LEARN_EVERY == 0:\n",
    "            for _ in range(LEARN_NUM):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.epsilon * self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # critic update\n",
    "        # step 1: \n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # step 2:\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # step 3:\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # step 4:\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # actor update\n",
    "        # step 5:\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # step 6:\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # step 7:\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "\n",
    "        # ---------------------------- update noise ---------------------------- #\n",
    "        self.epsilon -= EPSILON_DECAY\n",
    "        self.noise.reset()\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility that helps to implement Ornstein-Uhlenbeck noise for better exproration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, seed, mu=0., theta=OU_THETA, sigma=OU_SIGMA):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay buffer implementation that stores and generates tuples for network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size) \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 classes that implement Actor and Critic. These are enural networks with 2 hidden units each.\n",
    "\n",
    "Actor learns and returns a policy (best action for a given state).\n",
    "Critic learn Q value for a state-value pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    \n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.bn1(self.fc1(state)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xs = F.relu(self.bn1(self.fcs1(state)))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A utility function that initiates Unity environment, ad agent and starts the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=500, max_steps=1000, solved_score=30.0, max_last_scores=100,\n",
    "         actor_path='actor_model.pth', critic_path='critic_model.pth'):\n",
    "\n",
    "    last_scores = deque(maxlen=max_last_scores) \n",
    "    all_scores = [] \n",
    "\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations  \n",
    "        scores = np.zeros(num_agents) \n",
    "        agent.reset()\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            actions = agent.act(states, add_noise=True) \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations \n",
    "            rewards = env_info.rewards \n",
    "            dones = env_info.local_done \n",
    "\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)\n",
    "                \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            \n",
    "            if np.any(dones): \n",
    "                break\n",
    "    \n",
    "        average_score = np.mean(scores)\n",
    "        last_scores.append(average_score) \n",
    "        \n",
    "        last_average_scores = np.mean(average_score)\n",
    "        all_scores.append(last_average_scores) \n",
    "\n",
    "        print(\"\\repisode {}/{}, average score: {:.2f}\".format(i_episode, n_episodes, last_average_scores))\n",
    "\n",
    "        if last_average_scores >= solved_score:\n",
    "            print(\"\\nsolved in {} episodes\".format(i_episode))\n",
    "            \n",
    "            torch.save(agent.actor_local.state_dict(), actor_path)\n",
    "            torch.save(agent.critic_local.state_dict(), critic_path)\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "            plt.ylabel(\"score\")\n",
    "            plt.xlabel(\"episode #\")\n",
    "            plt.show()\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\tgoal_size -> 5.0\n\t\tgoal_speed -> 1.0\nUnity brain name: ReacherBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 33\n        Number of stacked Vector Observation: 1\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 4\n        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\nSize of each action: 4\nThere are 20 agents. Each observes a state with length: 33\nThe state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n -1.68164849e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mexxik/.virtualenvs/drl-project-2/lib/python3.5/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 1/500, average score: 0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 2/500, average score: 2.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 3/500, average score: 4.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 4/500, average score: 5.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 5/500, average score: 6.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 6/500, average score: 6.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 7/500, average score: 7.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 8/500, average score: 8.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 9/500, average score: 8.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 10/500, average score: 11.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 11/500, average score: 12.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 12/500, average score: 12.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 13/500, average score: 13.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 14/500, average score: 13.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 15/500, average score: 15.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 16/500, average score: 17.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 17/500, average score: 19.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 18/500, average score: 17.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 19/500, average score: 18.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 20/500, average score: 18.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 21/500, average score: 21.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 22/500, average score: 22.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 23/500, average score: 24.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 24/500, average score: 23.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 25/500, average score: 22.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 26/500, average score: 23.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 27/500, average score: 25.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 28/500, average score: 29.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 29/500, average score: 28.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 30/500, average score: 28.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 31/500, average score: 29.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\repisode 32/500, average score: 30.21\n\nsolved in 32 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"more_agents/Reacher.x86_64\")\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=1)\n",
    "ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\tgoal_speed -> 1.0\n\t\tgoal_size -> 5.0\nUnity brain name: ReacherBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 33\n        Number of stacked Vector Observation: 1\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 4\n        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\nSize of each action: 4\nThere are 20 agents. Each observes a state with length: 33\nThe state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n -1.68164849e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mexxik/.virtualenvs/drl-project-2/lib/python3.5/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 36.53449918339029\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"more_agents/Reacher.x86_64\")\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment\n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)    \n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=1)\n",
    "agent.load_actor_model(\"actor_model.pth\")\n",
    "\n",
    "while True:\n",
    "    actions = agent.act(states, add_noise=False)          # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Future Improvements.\n",
    "\n",
    "This project is an example Deep Deterministic Policy Gradient (DDPG) algorithm implemented for continuous control problem. Current implementation solved the enviroment in 32 episodes and shows consistent results during a test run.\n",
    "\n",
    "Here are some possible improvements and exploration paths:\n",
    "* Try another environment with 1 agent. It will generate less data and might require its own set of hyperparameters.\n",
    "* Explore other algorithms, like PPO or more advances and modern like suggested in the lesson (Trust Region Policy Optimization (TRPO) and Truncated Natural Policy Gradient (TNPG),  Distributed Distributional Deterministic Policy Gradients (D4PG))\n",
    "* Experiment with hyperparameters:\n",
    "\t* LEARN_EVERY and LEARN_NUM will allow to specify how ofter the network will be trained. Maybe try setting it to more \"agressive\" (frequest mode), even though it was reported that this approach is not stable.\n",
    "\t* Play with explotation/exploration by tweaking OU noise.\n",
    "\t* Previous lessons and projects showed that experience buffer size affect on the performance.\n",
    "\t* Would be interesting to try increasing batch size for training and adding more regularization (L2) to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
